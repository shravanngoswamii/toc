- year: '2022'
  data:
  - id: AllWenMarEtal22
    type: article
    cat: deep
    title: Sparse MoEs meet Efficient Ensembles
    journal: Transactions on Machine Learning Research
    year: '2022'
    url: https://openreview.net/forum?id=i0ZM36d2qU
    abstract: 'Machine learning models based on the aggregated outputs of submodels,
      either at the activation or prediction levels, often exhibit strong performance
      compared to individual models. We study the interplay of two popular classes
      of such models: ensembles of neural networks and sparse mixture of experts (sparse
      MoEs). First, we show that the two approaches have complementary features whose
      combination is beneficial. This includes a comprehensive evaluation of sparse
      MoEs in uncertainty related benchmarks. Then, we present efficient ensemble
      of experts (E3), a scalable and simple ensemble of sparse MoEs that takes the
      best of both classes of models, while using up to 45% fewer FLOPs than a deep
      ensemble. Extensive experiments demonstrate the accuracy, log-likelihood, few-shot
      learning, robustness, and uncertainty improvements of E3 over several challenging
      vision Transformer-based baselines. E3 not only preserves its efficiency while
      scaling to models with up to 2.7B parameters, but also provides better predictive
      performance and uncertainty estimates for larger models.'
    annote: <a href="https://github.com/google-research/vmoe">Code</a>
    author_formatted: James Urquhart Allingham, Florian Wenzel, Zelda E Mariet, Basil
      Mustafa, Joan Puigcerver, Neil Houlsby, Ghassen Jerfel, Vincent Fortuin, Balaji
      Lakshminarayanan, Jasper Snoek, Dustin Tran, Carlos Riquelme Ruiz, Rodolphe
      Jenatton
    editor_formatted: ''
  - id: AntJanAllEtal22
    type: inproceedings
    cat: approx deep
    title: Adapting the Linearised Laplace Model Evidence for Modern Deep Learning
    booktitle: 39th International Conference on Machine Learning
    series: Proceedings of Machine Learning Research
    volume: '162'
    pages: 796–821
    publisher: PMLR
    year: '2022'
    url: https://proceedings.mlr.press/v162/antoran22a.html
    abstract: The linearised Laplace method for estimating model uncertainty has received
      renewed attention in the Bayesian deep learning community. The method provides
      reliable error bars and admits a closed-form expression for the model evidence,
      allowing for scalable selection of model hyperparameters. In this work, we examine
      the assumptions behind this method, particularly in conjunction with model selection.
      We show that these interact poorly with some now-standard tools of deep learning–stochastic
      approximation methods and normalisation layers–and make recommendations for
      how to better adapt this classic method to the modern setting. We provide theoretical
      support for our recommendations and validate them empirically on MLPs, classic
      CNNs, residual networks with and without normalisation layers, generative autoencoders
      and transformers.
    author_formatted: Javier Antorán, David Janz, James Urquhart Allingham, Erik A.
      Daxberger, Riccardo Barbano, Eric T. Nalisnick, José Miguel Hernández-Lobato
    editor_formatted: Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári,
      Gang Niu, Sivan Sabato
- year: '2021'
  data:
  - id: AitYanObe21
    type: inproceedings
    title: Deep kernel processes
    booktitle: 38th International Conference on Machine Learning
    abstract: We define deep kernel processes in which positive definite Gram matrices
      are progressively transformed by nonlinear kernel functions and by sampling
      from (inverse) Wishart distributions. Remarkably, we find that deep Gaussian
      processes (DGPs), Bayesian neural networks (BNNs), infinite BNNs, and infinite
      BNNs with bottlenecks can all be written as deep kernel processes. For DGPs
      the equivalence arises because the Gram matrix formed by the inner product of
      features is Wishart distributed, and as we show, standard isotropic kernels
      can be written entirely in terms of this Gram matrix — we do not need knowledge
      of the underlying features. We define a tractable deep kernel process, the deep
      inverse Wishart process, and give a doubly-stochastic inducing-point variational
      inference scheme that operates on the Gram matrices, not on the features, as
      in DGPs. We show that the deep inverse Wishart process gives superior performance
      to DGPs and infinite BNNs on fully-connected baselines.
    year: '2021'
    cat: gp approx deep
    url: http://proceedings.mlr.press/v139/aitchison21a/aitchison21a.pdf
    author_formatted: Laurence Aitchison, Adam X. Yang, Sebastian W. Ober
    editor_formatted: ''
  - id: AntBhaAdeEtal20
    type: inproceedings
    cat: interpretability
    title: 'Getting a CLUE: A Method for Explaining Uncertainty Estimates'
    booktitle: 9th International Conference on Learning Representations
    year: '2021'
    month: April
    url: https://openreview.net/forum?id=XSLF1XFq5h
    abstract: Both uncertainty estimation and interpretability are important factors
      for trustworthy machine learning systems. However, there is little work at the
      intersection of these two areas. We address this gap by proposing a novel method
      for interpreting uncertainty estimates from differentiable probabilistic models,
      like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty
      Explanations (CLUE), indicates how to change an input, while keeping it on the
      data manifold, such that a BNN becomes more confident about the input's prediction.
      We validate CLUE through 1) a novel framework for evaluating counterfactual
      explanations of uncertainty, 2) a series of ablation experiments, and 3) a user
      study. Our experiments show that CLUE outperforms baselines and enables practitioners
      to better understand which input patterns are responsible for predictive uncertainty..
    author_formatted: Javier Antorán, Umang Bhatt, Tameem Adel, Adrian Weller, José
      Miguel Hernández-Lobato
    editor_formatted: ''
- year: '2020'
  data:
  - id: AntAllHer20
    type: inproceedings
    cat: approx deep
    title: Depth Uncertainty in Neural Networks
    booktitle: Advances in Neural Information Processing Systems 33
    year: '2020'
    url: https://proceedings.neurips.cc/paper/2020/hash/781877bda0783aac5f1cf765c128b437-Abstract.html
    abstract: Existing methods for estimating uncertainty in deep learning tend to
      require multiple forward passes, making them unsuitable for applications where
      computational resources are limited. To solve this, we perform probabilistic
      reasoning over the depth of neural networks. Different depths correspond to
      subnetworks which share weights and whose predictions are combined via marginalisation,
      yielding model uncertainty. By exploiting the sequential structure of feed-forward
      networks, we are able to both evaluate our training objective and make predictions
      with a single forward pass. We validate our approach on real-world regression
      and image classification tasks. Our approach provides uncertainty calibration,
      robustness to dataset shift, and accuracies competitive with more computationally
      expensive baselines.
    annote: <a href="https://github.com/cambridge-mlg/DUN">Code</a>
    author_formatted: Javier Antorán, James Urquhart Allingham, José Miguel Hernández-Lobato
    editor_formatted: Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina
      Balcan, Hsuan-Tien Lin
  - id: AshSoTebetal20
    type: article
    cat: gp approx
    title: Sparse Gaussian process variational autoencoders
    abstract: Large, multi-dimensional spatio-temporal datasets are omnipresent in
      modern science and engineering. An effective framework for handling such data
      are Gaussian process deep generative models (GP-DGMs), which employ GP priors
      over the latent variables of DGMs. Existing approaches for performing inference
      in GP-DGMs do not support sparse GP approximations based on inducing points,
      which are essential for the computational efficiency of GPs, nor do they handle
      missing data – a natural occurrence in many spatio-temporal datasets – in a
      principled manner. We address these shortcomings with the development of the
      sparse Gaussian process variational autoencoder (SGP-VAE), characterised by
      the use of partial inference networks for parameterising sparse GP approximations.
      Leveraging the benefits of amortised variational inference, the SGP-VAE enables
      inference in multi-output sparse GPs on previously unobserved data with no additional
      training. The SGP-VAE is evaluated in a variety of experiments where it outperforms
      alternative approaches including multi-output GPs and structured VAEs.
    url: https://arxiv.org/abs/2010.10177
    year: '2020'
    author_formatted: Matthew Ashman, Jonny So, Will Tebbutt, Vincent Fortuin, Michael
      Pearce, Richard E. Turner
    editor_formatted: ''
- year: '2019'
  data:
  - id: AdeValGhaWel19
    type: inproceedings
    cat: deep fairness
    title: One-network Adversarial Fairness
    booktitle: 33rd AAAI Conference on Artificial Intelligence
    year: '2019'
    month: January
    address: Hawaii
    url: http://mlg.eng.cam.ac.uk/adrian/AAAI2019_OneNetworkAdversarialFairness.pdf
    abstract: There is currently a great expansion of the impact of machine learning
      algorithms on our lives, prompting the need for objectives other than pure performance,
      including fairness. Fairness here means that the outcome of an automated decision-making
      system should not discriminate between subgroups characterized by sensitive
      attributes such as gender or race. Given any existing differentiable classifier,
      we make only slight adjustments to the architecture including adding a new hidden
      layer, in order to enable the concurrent adversarial optimization for fairness
      and accuracy. Our framework provides one way to quantify the tradeoff between
      fairness and accuracy, while also leading to strong empirical performance.
    author_formatted: Tameem Adel, Isabel Valera, Zoubin Ghahramani, Adrian Weller
    editor_formatted: ''
  - id: AdeWel19
    type: inproceedings
    cat: rl gm approx
    title: 'TibGM: A Transferable and Information-Based Graphical Model Approach for
      Reinforcement Learning'
    booktitle: 36th International Conference on Machine Learning
    year: '2019'
    month: June
    address: Long Beach
    url: http://mlg.eng.cam.ac.uk/adrian/ICML2019-TibGM.pdf
    abstract: One of the challenges to reinforcement learning (RL) is scalable transferability
      among complex tasks. Incorporating a graphical model (GM), along with the rich
      family of related methods, as a basis for RL frameworks provides potential to
      address issues such as transferability, generalisation and exploration. Here
      we propose a flexible GM-based RL framework which leverages efficient inference
      procedures to enhance generalisation and transfer power. In our proposed transferable
      and information-based graphical model framework ‘TibGM’, we show the equivalence
      between our mutual information-based objective in the GM, and an RL consolidated
      objective consisting of a standard reward maximisation target and a generalisation/transfer
      objective. In settings where there is a sparse or deceptive reward signal, our
      TibGM framework is flexible enough to incorporate exploration bonuses depicting
      intrinsic rewards. We empirically verify improved performance and exploration
      power.
    author_formatted: Tameem Adel, Adrian Weller
    editor_formatted: ''
- year: '2018'
  data:
  - id: AdeGhaWell18
    type: inproceedings
    cat: deep interpretability
    title: Discovering interpretable representations for both deep generative and
      discriminative models
    booktitle: 35th International Conference on Machine Learning
    year: '2018'
    month: July
    address: Stockholm Sweden
    url: http://mlg.eng.cam.ac.uk/adrian/ICML18-Discovering.pdf
    abstract: Interpretability of representations in both deep generative and discriminative
      models is highly desirable. Current methods jointly optimize an objective combining
      accuracy and interpretability. However, this may reduce accuracy, and is not
      applicable to already trained models. We propose two interpretability frameworks.
      First, we provide an interpretable lens for an existing model. We use a generative
      model which takes as input the representation in an existing (generative or
      discriminative) model, weakly supervised by limited side information. Applying
      a flexible and invertible transformation to the input leads to an interpretable
      representation with no loss in accuracy. We extend the approach using an active
      learning strategy to choose the most useful side information to obtain, allowing
      a human to guide what “interpretable" means. Our second framework relies on
      joint optimization for a representation which is both maximally informative
      about the side information and maximally compressive about the non-interpretable
      data factors. This leads to a novel perspective on the relationship between
      compression and regularization. We also propose a new interpretability evaluation
      metric based on our framework. Empirically, we achieve state-of-the-art results
      on three datasets using the two proposed algorithms.
    author_formatted: Tameem Adel, Zoubin Ghahramani, Adrian Weller
    editor_formatted: ''
  - id: AhnCheShiWel18
    type: inproceedings
    cat: gm approx
    title: Gauged Mini-Bucket Elimination for Approximate Inference
    booktitle: 21st International Conference on Artificial Intelligence and Statistics
    year: '2018'
    month: April
    address: Playa Blanca, Lanzarote, Canary Islands
    abstract: Computing the partition function Z of a discrete graphical model is
      a fundamental inference challenge. Since this is computationally intractable,
      variational approximations are often used in practice. Recently, so-called gauge
      transformations were used to improve variational lower bounds on Z. In this
      paper, we propose a new gauge-variational approach, termed WMBE-G, which combines
      gauge transformations with the weighted mini-bucket elimination (WMBE) method.
      WMBE-G can provide both upper and lower bounds on Z, and is easier to optimize
      than the prior gauge-variational algorithm. We show that WMBE-G strictly improves
      the earlier WMBE approximation for symmetric models including Ising models with
      no magnetic field. Our experimental results demonstrate the effectiveness of
      WMBE-G even for generic, nonsymmetric models.
    url: http://mlg.eng.cam.ac.uk/adrian/Gauge_for_Holder_Inference.pdf
    author_formatted: Sungsoo Ahn, Michael Chertkov, Jinwoo Shin, Adrian Weller
    editor_formatted: ''